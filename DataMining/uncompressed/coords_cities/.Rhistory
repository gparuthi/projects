tdf[1]
tdf[2]
tdf[3]
delhi.df
getGroupedDF(delhi.df)
tdf  = getGroupedDF(delhi.df)
tdf
tdf[1]
tdf[,1]
tdf[:]
tdf[1:]
dim(tdf)
tdf[1,]
cbind(c(1,2,3,4,5))
cbind(tdf, c(1,2,3,4,5))
tr = c(1,2,3,4,5)
dim(tr)
tr
dim(tdf)
cbind(t(tdf), c(1,2,3,4,5))
cbind(t(tdf), c(16,25,34,33,11))
cbind(tdf, t(c(16,25,34,33,11)))
tdf[1,]
tdf[2,]
tdf <- cbind(tdf, t(c(16,25,34,33,11)))
tdf[2.]
tdf[2,]
tdf[1,]
dim(tdf)
tdf  = getGroupedDF(delhi.df)
dim(tdf)
tdf <- rbind(tdf, t(c(16,25,34,33,11)))
tdf
score.sentiment(t.text, pos.words, neg.words,.progress='text')
score.sentiment(tdf[1,], pos.words, neg.words,.progress='text')
score.sentiment(tdf[1,], pos.words, neg.words,.progress='text')$score
t(score.sentiment(tdf[1,], pos.words, neg.words,.progress='text')$score)
rbind(tdf,t(score.sentiment(tdf[1,], pos.words, neg.words,.progress='text')$score))
tdf
source('~/.active-rstudio-document')
getSentiment(tdf)
source('~/.active-rstudio-document')
getSentiment(tdf)
tdf <- getSentiment(tdf)
tdf[3,]
hist(tdf)
hist(tdf[3,])
hist(tdf[,1])
hist(as.numeric(tdf[3,]))
save.image("~/Documents/Work/PatternsInCommunity/support_projects/twitter-sentiment-analysis-tutorial-201107/1.RData")
hist(as.numeric(tdf[3,]))
hist(as.numeric(tdf[,1]),"days", format="%d %b")
hist((tdf[,1]),"days", format="%d %b")
tdf[,1]
tdf[1,]
hist((tdf[1,]),"days", format="%d %b")
source('~/.active-rstudio-document')
delhi.df <- getTweetsDF()
source('~/Documents/Work/PatternsInCommunity/support_projects/twitter-sentiment-analysis-tutorial-201107/R/getPlaceTweets.R')
getTweets()
source('~/Documents/Work/PatternsInCommunity/support_projects/twitter-sentiment-analysis-tutorial-201107/R/getPlaceTweets.R')
getTweetsDF(getTweets())
delhi.tweets <- getTweets()
delhi.df <- getTweetsDF(delhi.tweets)
delhi.df <- getSentiment(delhi.df)
delhi.df
delhi.gpd <- getGroupedDF(delhi.df)
delhi.gpd
delhi.gpd <- getSentiment(delhi.gpd)
delhi.gpd
getHistForDays(delhi.tweets)
delhi.tweets
tweets = delhi.tweets
tweets.dates <- lapply(tweets, getTweetDF)
tweets.tdf <- ldply(tweets.dates)
tweets.tdf
hist(tweets.tdf[,1], "days", format = "%d %b")
tweets.tdf[,1]
tweets.tdf[,2]
hist(tweets.tdf[,2], "days", format = "%d %b")
hist(tweets.tdf[,1], "days", format = "%d %b")
hist(as.Date(tweets.tdf[,1]), "days", format = "%d %b")
hist(as.Date(tweets.tdf[,1],origin="1970-1-1"), "days", format = "%d %b")
hist(as.Date(tweets.tdf[,2],origin="1970-1-1"), "days", format = "%d %b")
tweets.dates <- lapply(tweets, function(t) as.Date(t$getCreated()))
tweets.dates
tweets.tdf <- ldply(tweets.dates)
hist(tweets.tdf[,1], "days", format = "%d %b")
summarise(tweets.tdf)
summarise(tweets.dates)
tweets.tdf
summarise(tweets.tdf)
summary(tweets.tdf)
hist(tweets.tdf[,1], "days", format = "%d %b")
history(199)
hist(delhi.df[,1], "days", format = "%d %b")
source('~/.active-rstudio-document')
hist(delhi.df[,1], "days", format = "%d %b")
getHistForDays(delhi.tweets)
getHistForDays(delhi.tweets)
GenWordCloud(tdf=delhi.df)
source('~/Documents/Work/PatternsInCommunity/support_projects/twitter-sentiment-analysis-tutorial-201107/R/wordCloud_twitter.R')
GenWordCloud(tdf=delhi.df)
delhi.df
GenWordCloud(tdf=delhi.df)
tdf = delhi.df
ap.corpus <- Corpus(DataframeSource(data.frame(rbind(tdf)[1,])))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, tolower)
ap.corpus <- tm_map(ap.corpus, function(x) removeWords(x, stopwords("english")))
ap.tdm <- TermDocumentMatrix(ap.corpus)
ap.m <- as.matrix(ap.tdm)
ap.v <- sort(rowSums(ap.m),decreasing=TRUE)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)
table(ap.d$freq)
pal2 <- brewer.pal(8,"Dark2")
png("wordcloud_packages.png", width=1280,height=800)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
tdf[,1]
rbind(tdf)[1,]
rbind(tdf)[,1]
tdf[,1]
source('~/.active-rstudio-document')
GenWordCloud(delhi.df)
delhi.df
GenWordCloud(delhi.gpd)
tdf = delhi.gpd
tdf
tdf[,1]
tdf[1,]
ap.corpus <- Corpus(DataframeSource(data.frame((tdf)[1,])))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, tolower)
ap.corpus <- tm_map(ap.corpus, function(x) removeWords(x, stopwords("english")))
ap.tdm <- TermDocumentMatrix(ap.corpus)
ap.m <- as.matrix(ap.tdm)
ap.v <- sort(rowSums(ap.m),decreasing=TRUE)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)
table(ap.d$freq)
pal2 <- brewer.pal(8,"Dark2")
png("wordcloud_packages.png", width=1280,height=800)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
source('~/.active-rstudio-document')
t_list = delhi.gpd(1,)
delhi.gpd(1,)
delhi.gpd(,1)
delhi.gpd
delhi.gpd[,1]
delhi.gpd[1,]
t_list = delhi.gpd[1,]
ap.corpus <- Corpus(DataframeSource(data.frame(t_list)))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, tolower)
ap.corpus <- tm_map(ap.corpus, function(x) removeWords(x, stopwords("english")))
ap.tdm <- TermDocumentMatrix(ap.corpus)
ap.m <- as.matrix(ap.tdm)
ap.v <- sort(rowSums(ap.m),decreasing=TRUE)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)
table(ap.d$freq)
pal2 <- brewer.pal(8,"Dark2")
png("wordcloud_packages.png", width=1280,height=800)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
}
source('~/Documents/Work/PatternsInCommunity/support_projects/twitter-sentiment-analysis-tutorial-201107/R/wordCloud_twitter.R')
GenWordCloud(delhi.gpd[1,])
delhi.df[1,]
delhi.df[,1]
GenWordCloud(delhi.df[,1])
mey.tweets <- searchTwitter('@margareteyoung',n=1000)
mey.tweets
mey.tweets[1]
mey.tweets[2]
mey.tweets <- searchTwitter('@umsi',n=1000)
mey.tweets
GenWordCloud(delhi.df[1,])
GenWordCloud(delhi.df[,1])
mey.tweets
mey.df <- getTweetDF(mey.tweets)
mey.df <- getTweetsDF(mey.tweets)
mey.gpd <- getGroupedDF(mey.df)
mey.gpd <- getSentiment(mey.gpd)
mey.gpd
getHistForDays(mey.tweets)
GenWordCloud(mey.df[,1])
history()
mey.tweets <- searchTwitter('@sabarish',n=1000)
mey.tweets
mey.tweets <- searchTwitter('sandy',n=1000)
mey.df <- getTweetsDF(mey.tweets)
mey.gpd <- getGroupedDF(mey.df)
mey.gpd <- getSentiment(mey.gpd)
getHistForDays(mey.tweets)
mey.tweets
mey.gpd
getHistForDays(mey.tweets)
mey.df
GenWordCloud(mey.df[,1])
mey.tweets
laply(mey.tweets, function(t) t$getText())
mey->text = laply(mey.tweets, function(t) t$getText())
GenWordCloud(mey->text)
GenWordCloud(mey.text)
mey.text = laply(mey.tweets, function(t) t$getText())
GenWordCloud(mey.text)
mey.text
GenWordCloud(mey.text)
t_list = mey.text
ap.corpus <- Corpus(DataframeSource(data.frame(t_list)))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, tolower)
ap.corpus <- tm_map(ap.corpus, function(x) removeWords(x, stopwords("english")))
ap.tdm <- TermDocumentMatrix(ap.corpus)
ap.m <- as.matrix(ap.tdm)
ap.v <- sort(rowSums(ap.m),decreasing=TRUE)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)
table(ap.d$freq)
pal2 <- brewer.pal(8,"Dark2")
png("wordcloud_packages.png", width=1280,height=800)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
}
png("wordcloud_packages.png", width=4280,height=3000)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,1),min.freq=3, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
wordcloud(ap.d$word,ap.d$freq, scale=c(20,.2),min.freq=3, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
setwd("~/Documents/Work/PatternsInCommunity/projects/DataMining/uncompressed/cities")
library("rjson")
library(plyr)
text <- readLines("delhi.1")
json = fromJSON(paste ('[',paste(text,collapse=',', sep=',') ,']'))
json
dim(json)
json[1]
json[2]
json[3]
json[4]
json[5]
json[1,
]
dim(json)
dim(json[1])
dim(json[1,])
json
summarise(json)
summarize(json)
summary(json)
json[1]
json[1]$coordinates$coordinates
json[[1]$coordinates$coordinates
json[[1]]$coordinates$coordinates
af = NULL
infile = "delhi.1"
con  <- file(inputFile, open = "r")
dataList <- list()
con  <- file(infile, open = "r")
while (length(oneLine <- readLines(con, n = 1, warn = FALSE)) > 0) {
af <- rbind(af, fromJSON(oneLine))
}
af
af$coordinates
af[1,]
af[1,]$coordinates
af[]$coordinates
af$coordinates
af[1]
af[2]
af[3]
af[4]
af[5]
af[5,]
af[1,]
json[1,]
json[1]
json[[1]]$coordinates$coordinates
json[[2]]$coordinates$coordinates
json[[2]]$txt
json[[2]]$text
json[[2]][1]
json[[2]][2]
json[[2]][1]
json[1,][1]
json[1]
json[1][1]
json[[1][1]
json[[1]][1]
dimnames(json)
dimnames(json[1])
dimnames(json[[1])
dimnames(json[[1]])
names(json)
names(json[[1]])
length(json)
ind =0
while(length(json)){}
while(length(json)){
int +=1
json[1]$temp = 'temp'
c(1:length(json))
rbind(json,c(1:length(json)))
json[2]
json[2,1]
json[2,2]
json[2,1]
json[1,2]
json[1,1]
json[1]
json
rbind(json,c(1:length(json)))[1]
rbind(json,c(1:length(json)))[1,2]
rbind(json,c(1:length(json)))[1,1]
rbind(json,c(1:length(json)))[2,1]
rbind(json,c(1:length(json)))[2,2]
rbind(json,c(1:length(json)))[2,4]
t(json)
(json)
json[1]
json[1,1
]
cbind(json,c(1:length(json)))
laply(json, function(t) t[1])
laply(json, function(t) t[1][1])
laply(json, function(t) t[1]$text)
laply(json, function(t) t$text)
cbind(json, laply(json, function(t) t$text))
delhi.data -> cbind(json, laply(json, function(t) t$text))
delhi.data = cbind(json, laply(json, function(t) t$text))
delhi.data
delhi.data[1]
delhi.data[[1]]
delhi.data[1]
delhi.data
GenWordCloud(delhi.data$text)
delhi.data
delhi.data[2]
delhi.data[1,2]
delhi.data[,2]
aa = delhi.data[,2]
aa
delhi.data[[2]]
delhi.data[,[2]]
delhi.data[,[1]]
delhi.data[1,2]
delhi.data[1,2][1]
delhi.data[1,2]
delhi.data[1,]
delhi.data[1,2]
delhi.data[1,2]
delhi.data[,2]
as.list(delhi.data[,2])
as.vector(delhi.data[,2])
GenWordCloud(delhi.data[,2])
GenWordCloud(laply(json, function(t) t$text))
ls
ls()
history(1000)
nyfile = 'ny.1'
con  <- file(infile, open = "r")
con  <- file(nyfile, open = "r")
text <- readlines('ny.1')
text <- readLines('ny.1')
json = fromJSON(paste ('[',paste(text,collapse=',', sep=',') ,']'))
clear
rm(ny.*)
nyc.data = cbind(json, laply(json, function(t) t$text))
nyc.data
GenWordCloud(laply(json, function(t) t$text))
ap.d$freq
ap.d
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
png("nyc_11_1_12.png", width=1280,height=800)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
ap.d[1]
ap.d[1,]
ap.d[1,1]
ap.d[1,2]
ap.d[1:100,]
pal2 <- brewer.pal(8,"Dark2")
ap.corpus <- Corpus(DataframeSource(data.frame(t_list)))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, tolower)
ap.corpus <- tm_map(ap.corpus, function(x) removeWords(x, stopwords("english")))
ap.tdm <- TermDocumentMatrix(ap.corpus)
ap.m <- as.matrix(ap.tdm)
ap.v <- sort(rowSums(ap.m),decreasing=TRUE)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)
table(ap.d$freq)
pal2 <- brewer.pal(8,"Dark2")
png("nyc_11_1_12.png", width=1280,height=800)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=5, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
ap.d[1:100,]
library(ggplot2)
g.hist = ggplot(data=ap.d[1:10])
g.hist = ggplot(data=ap.d)
print(g.hist)
g.hist = ggplot(data=ap.d, mapping=aes(x=word, fill=freq))
print(g.hist)
g.hist = g.hist + geom_bar( binwidth=1 )
print(g.hist)
qplot(ap.d[1:10]$word, ap.d[1:10]$freq)
ap.d[1:10]$word
ap.d[1:10]$words
ap.d[1:10]
ap.d
ap.d[1]
ap.d[1,]
ap.d[1:10,]
qplot(ap.d[1:10,]$word, ap.d[1:10]$freq)
ap.d[1:10,]$word
qplot(ap.d[1:10,]$word, ap.d[1:10,]$freq)
dd = ap.d[1:10,]
qplot(dd$word,dd$freq)
dd$word
warnings()
plot(dd$word,dd$freq)
plot(dd$word,dd$freq,type=)
barplot(dd$word,dd$freq,type=)
barplot(dd$word,dd$freq)
ap[1,]
ap.d[1,]
ap.d[ap.d$word == 'obama']
ap.d[ap.d$word == 'obama',]
ap.d[ap.d$word == 'sandy',]
ap.d[ap.d$word == 'politics',]
ap.d[ap.d$word == 'election',]
ap.d[ap.d$word == 'elections',]
nyc.json = json
text <- readLines("delhi.1")
json = fromJSON(paste ('[',paste(text,collapse=',', sep=',') ,']'))
delhi.json = json
GenWordCloud(laply(json, function(t) t$text))
ap.d
ap.d[1,]
ap.corpus <- Corpus(DataframeSource(data.frame(t_list)))
t_list = laply(json, function(t) t$text)
t_list
ap.corpus <- Corpus(DataframeSource(data.frame(t_list)))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, tolower)
ap.corpus <- tm_map(ap.corpus, function(x) removeWords(x, stopwords("english")))
ap.tdm <- TermDocumentMatrix(ap.corpus)
ap.v <- sort(rowSums(ap.m),decreasing=TRUE)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)
table(ap.d$freq)
pal2 <- brewer.pal(8,"Dark2")
png("nyc_11_1_12.png", width=1280,height=800)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=5, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
ap.d
ap.d[1,]
delhi.data
delhi.data$text
delhi.data[,2]
t_list = delhi.data[,2]
t_list
ap.corpus <- Corpus(DataframeSource(data.frame(t_list)))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, tolower)
ap.corpus <- tm_map(ap.corpus, function(x) removeWords(x, stopwords("english")))
ap.tdm <- TermDocumentMatrix(ap.corpus)
ap.m <- as.matrix(ap.tdm)
ldply(t_list)
t_list = ldply(t_list)
ap.corpus <- Corpus(DataframeSource(data.frame(t_list)))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, tolower)
ap.corpus <- tm_map(ap.corpus, function(x) removeWords(x, stopwords("english")))
ap.tdm <- TermDocumentMatrix(ap.corpus)
ap.m <- as.matrix(ap.tdm)
ap.v <- sort(rowSums(ap.m),decreasing=TRUE)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)
table(ap.d$freq)
pal2 <- brewer.pal(8,"Dark2")
png("nyc_11_1_12.png", width=1280,height=800)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=5, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
png("del_11_1_12.png", width=1280,height=800)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
t_list = ldply(nyc.data[,2])
t_list
ap.corpus <- Corpus(DataframeSource(data.frame(t_list)))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, tolower)
iconvlist(t_list)
iconvlist()
iconv(t_list)
t_list.a <- iconv(t_list)
t_list.a
t_list = ldply(nyc.data[,2],.fun=iconv)
t_list
ap.corpus <- Corpus(DataframeSource(data.frame(t_list)))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, tolower)
ap.corpus <- tm_map(ap.corpus, function(x) removeWords(x, stopwords("english")))
ap.tdm <- TermDocumentMatrix(ap.corpus)
ap.m <- as.matrix(ap.tdm)
ap.v <- sort(rowSums(ap.m),decreasing=TRUE)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)
table(ap.d$freq)
pal2 <- brewer.pal(8,"Dark2")
png("nyc_11_1_12.png", width=1280,height=800)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3, max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
ap.d[1:100,]
length(ap.d[1,])
length(ap.d[1:,])
length(ap.d[1:])
length(ap.d)
length(ap.d[1])
length(ap.d[,1])

'''
Created on Dec 2, 2012

@author: gparuthi
'''

import nltk
from ujson import loads, dumps
from datetime import datetime
import os
from log import logger
from city import City
from dateutil.parser import parse

DELF_PATH = '/Users/gaurav/Documents/Work/Projects/DataMining/uncompressed/locations_cities/'+'del_11_1_to_11_15'+'.data'
NYF_PATH = '/Users/gaurav/Documents/Work/Projects/DataMining/uncompressed/locations_cities/'+'ny_11_1_to_11_15'+'.data'
logo = logger('City_Location')

def log(str):
    logo.log(str)


def writeJsonToFile(fpath,json):
    if not os.path.isfile(fpath):
        f= open(fpath,'wb')
        f.write(dumps(json))
        f.close()
    else:
        print 'file already exists: '+ fpath    

def get_nouns(text, city):
    sentences = nltk.sent_tokenize(text) # NLTK default sentence segmenter
    sentences = [nltk.word_tokenize(sent) for sent in sentences] # NLTK word tokenizer
    #sentences = post_tag(sentences)
    #print sentences
    for sent in sentences:
        a = nltk.pos_tag(sent) 
        for w in a:
            for w in a:
                if w[1][0] == 'N':
                    if w[0] in city.tdf:
                        city.tdf[w[0]] +=1
                    else:
                        city.tdf[w[0]] = 1
    return city.tdf

def get_timeline(text, date, city):
    sentences = nltk.sent_tokenize(text) # NLTK default sentence segmenter
    sentences = [nltk.word_tokenize(sent) for sent in sentences] # NLTK word tokenizer
    #sentences = post_tag(sentences)
    #print sentences
    for sent in sentences:
        #print sent
        #a = nltk.pos_tag(sent) 
        for w in sent:
            if w in city.tdf_nouns:
                if date in city.timeline:
                    city.timeline[date].append(w) # add word to that date key
                else:
                    city.timeline[date] = []
    return city.tdf

def get_all(text, city):
    sentences = nltk.sent_tokenize(text) # NLTK default sentence segmenter
    sentences = [nltk.word_tokenize(sent) for sent in sentences] # NLTK word tokenizer
    #sentences = post_tag(sentences)
    #print sentences
    for sent in sentences:
        #print sent
        #a = nltk.pos_tag(sent) 
        for w in sent:
            if w in city.tdf:
                city.tdf[w] +=1
            else:
                city.tdf[w] = 1
    return city.tdf

def analyze_city(f, loc):
    log('Processing file: ' + f.name)
    # define vars
    start_time = datetime.now()
    loc_lines = 0
    tot_lines = 1
    line = f.readline()
    while line:
        rec = loads(line)
        noun_list = get_nouns(rec['text'], loc)
        if tot_lines %1000 ==0:
            log(f.name+ '::tot_lines: ' + str(tot_lines))
        tot_lines+=1
        line = f.readline()
    log(f.name + '::tot_lines: ' + str(tot_lines))

def getTextCity(f):
    log('Processing file: ' + f.name)
    # define vars
    loc_lines = 0
    tot_lines = 1
    line = f.readline()
    text = ''
    while line:
        rec = loads(line)
        text += rec['text']
        if tot_lines %10000 ==0:
            log(f.name+ '::tot_lines: ' + str(tot_lines))
            return text
        tot_lines+=1
        line = f.readline()
    logo.log_file_stats(f.name, tot_lines, tot_lines)
    return text

def getTillHour(date):
    ret = parse(date).replace(minute=0,second=0,tzinfo=None)
    return ret

def getTimesCity(f, city):
    log('Processing file: ' + f.name)
    # define vars                                                                                                                                               start_time = datetime.now()
    loc_lines = 0
    tot_lines = 1
    line = f.readline()
    text = ''
    while line:
        rec = loads(line)
        text = rec['text'].encode('utf-8')
        # get pos_tags for the line and write to output file
        tweet_date = getTillHour(rec['created_at'])
        get_timeline(text, tweet_date, city)
        if tot_lines %1000 ==0:
            log(f.name+ '::tot_lines: ' + str(tot_lines))
        tot_lines+=1
        line = f.readline()
    logo.log_file_stats(f.name, tot_lines, tot_lines)
    return city

def getLineCity(f, city):
    log('Processing file: ' + f.name)
    # define vars 
    start_time = datetime.now()
    loc_lines = 0
    tot_lines = 1
    line = f.readline()
    text = ''
    while line:
        rec = loads(line)
        text = rec['text'].encode('utf-8')
        # get pos_tags for the line and write to output file
        tweet_date = getTillHour(rec['created_at'])
        get_all(text, tweet_date, city)
        if tot_lines %1000 ==0:
            log(f.name+ '::tot_lines: ' + str(tot_lines))
        tot_lines+=1
        line = f.readline()
    logo.log_file_stats(f.name, tot_lines, tot_lines)
    city.tdf_class = post_tag(city.tdf)
    # get only nouns and those not in the stop words list
    city.clean_tdf()
    return city


def post_tag(intdf):
    log('starting pos_tag analysis on the final tdf')
    i=0
    posts = nltk.pos_tag(intdf.keys())
    log('Done')
    return dict((x,y) for x,y in posts)

    
        
#delc = City('delhi',None)
#nyc = City('nyc', None)

if __name__ == '__main__':
    print 'in main'
    #analyze_city(open(DELF_PATH), delc)
